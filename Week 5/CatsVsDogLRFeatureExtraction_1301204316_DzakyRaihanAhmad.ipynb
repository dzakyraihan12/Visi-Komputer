{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#DZAKY RAIHAN AHMAD\n",
        "#1301204316\n",
        "#MENGGUNAKAN METODE LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "KKIv4W5VFR0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnb3caULgM1f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from skimage.feature import hog\n",
        "from skimage import exposure"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS1iBospgSjb",
        "outputId": "d8d0b042-cba1-442a-8eb9-b447fba70e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-05 03:10:15--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.101.207, 142.250.141.207, 142.251.2.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.101.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   136MB/s    in 0.5s    \n",
            "\n",
            "2023-11-05 03:10:16 (136 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TANPA MENGGUNAKAN FEATURE EXTRACTION"
      ],
      "metadata": {
        "id": "D-cQ60eHgU2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data\n",
        "image_paths = []  # List of image file paths\n",
        "labels = []  # List of corresponding labels (0 for cats, 1 for dogs)\n",
        "\n",
        "image_size = (150, 150)  # Define the desired image size\n",
        "\n",
        "for img_path in os.listdir(train_cats_dir):\n",
        "    image = cv2.imread(os.path.join(train_cats_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.resize(image, image_size)  # Resize the image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
        "        image = image / 255.0  # Normalize pixel values (if needed)\n",
        "        image = image.flatten()  # Flatten the image into a 1D array\n",
        "        image_paths.append(image)\n",
        "        labels.append(0)  # 0 represents cats\n",
        "\n",
        "for img_path in os.listdir(train_dogs_dir):\n",
        "    image = cv2.imread(os.path.join(train_dogs_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.resize(image, image_size)  # Resize the image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
        "        image = image / 255.0  # Normalize pixel values (if needed)\n",
        "        image = image.flatten()  # Flatten the image into a 1D array\n",
        "        image_paths.append(image)\n",
        "        labels.append(1)  # 1 represents dogs\n",
        "\n",
        "# Convert the image_paths and labels lists to NumPy arrays\n",
        "X = np.array(image_paths)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define a parameter grid to search\n",
        "\"\"\"\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Different values of C\n",
        "    'penalty': ['l1', 'l2'],  # L1 and L2 regularization\n",
        "    'max_iter': [1000],\n",
        "    'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']  # Different solvers\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(C=0.1, max_iter=1000, penalty='l1', solver= 'saga')\n",
        "\n",
        "\"\"\"\n",
        "grid_search = GridSearchCV(logistic_regression, param_grid, cv=4)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\"\"\"\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', logistic_regression)\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "#logistic_regression.fit(X_train, y_train)\n",
        "#y_pred = logistic_regression.predict(X_test)\n",
        "# Calculate metrics for the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Test Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Test Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Test F1: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "I5vdPgU7gXTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca0147b-4b23-48a2-82c9-067501802687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 50.50%\n",
            "Test Precision: 50.79%\n",
            "Test Recall: 48.26%\n",
            "Test F1: 49.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "# Load and preprocess the validation data\n",
        "image_paths = []  # List of image file paths\n",
        "labels = []  # List of corresponding labels (0 for cats, 1 for dogs)\n",
        "\n",
        "for img_path in os.listdir(validation_cats_dir):\n",
        "    image = cv2.imread(os.path.join(validation_cats_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.resize(image, image_size)  # Resize the image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
        "        image = image / 255.0  # Normalize pixel values\n",
        "        image = image.flatten()  # Flatten the image into a 1D array\n",
        "        image_paths.append(image)\n",
        "        labels.append(0)  # 0 represents cats\n",
        "\n",
        "for img_path in os.listdir(validation_dogs_dir):\n",
        "    image = cv2.imread(os.path.join(validation_dogs_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.resize(image, image_size)  # Resize the image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
        "        image = image / 255.0  # Normalize pixel values\n",
        "        image = image.flatten()  # Flatten the image into a 1D array\n",
        "        image_paths.append(image)\n",
        "        labels.append(1)  # 1 represents dogs\n",
        "\n",
        "# Convert the image_paths and labels lists to NumPy arrays\n",
        "X_validation = np.array(image_paths)\n",
        "y_validation = np.array(labels)\n",
        "\n",
        "# Use your trained Logistic Regression model to make predictions on the validation data\n",
        "y_validation_pred = logistic_regression.predict(X_validation)\n",
        "\n",
        "# Calculate metrics for the validation data\n",
        "accuracy_validation = accuracy_score(y_validation, y_validation_pred)\n",
        "precision_validation = precision_score(y_validation, y_validation_pred)\n",
        "recall_validation = recall_score(y_validation, y_validation_pred)\n",
        "f1_validation = f1_score(y_validation, y_validation_pred)\n",
        "\n",
        "print(f\"Validation Accuracy: {accuracy_validation * 100:.2f}%\")\n",
        "print(f\"Validation Precision: {precision_validation * 100:.2f}%\")\n",
        "print(f\"Validation Recall: {recall_validation * 100:.2f}%\")\n",
        "print(f\"Validation F1: {f1_validation * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "Fe-yJzhcgaQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563c71b6-bb7a-4264-f793-4dfab13889ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 51.30%\n",
            "Validation Precision: 51.22%\n",
            "Validation Recall: 54.60%\n",
            "Validation F1: 52.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MENGGUNAKAN FEATURE EXTRACTION HOG"
      ],
      "metadata": {
        "id": "fl7KuyX1gYoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract HOG features from an image\n",
        "def extract_hog_features(image):\n",
        "    # Compute HOG features\n",
        "    features, hog_image = hog(image, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
        "    return features\n",
        "\n",
        "# Load and preprocess the data\n",
        "image_paths = []  # List of image file paths\n",
        "labels = []  # List of corresponding labels (0 for cats, 1 for dogs)\n",
        "\n",
        "for img_path in os.listdir(train_cats_dir):\n",
        "    image = cv2.imread(os.path.join(train_cats_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.resize(image, (150, 150))  # Resize the image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
        "        features = extract_hog_features(image)  # Extract HOG features\n",
        "        image_paths.append(features)\n",
        "        labels.append(0)  # 0 represents cats\n",
        "\n",
        "for img_path in os.listdir(train_dogs_dir):\n",
        "    image = cv2.imread(os.path.join(train_dogs_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.resize(image, (150, 150))  # Resize the image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
        "        features = extract_hog_features(image)  # Extract HOG features\n",
        "        image_paths.append(features)\n",
        "        labels.append(1)  # 1 represents dogs\n",
        "\n",
        "\n",
        "# Convert the image_paths and labels lists to NumPy arrays\n",
        "X = np.array(image_paths)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define a parameter grid to search\n",
        "\"\"\"\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Different values of C\n",
        "    'penalty': ['l1', 'l2'],  # L1 and L2 regularization\n",
        "    'max_iter': [1000],\n",
        "    'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']  # Different solvers\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(C=0.1, max_iter=1000, penalty='l1', solver= 'saga')\n",
        "\n",
        "\"\"\"\n",
        "grid_search = GridSearchCV(logistic_regression, param_grid, cv=4)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_test_pred = best_model.predict(X_test)\n",
        "\"\"\"\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', logistic_regression)\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "#logistic_regression.fit(X_train, y_train)\n",
        "#y_pred = logistic_regression.predict(X_test)\n",
        "# Calculate metrics for the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Test Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Test Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Test F1: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkPcoDZ0uPxq",
        "outputId": "66c713a9-d373-42aa-d3c2-969c1677d825"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 68.25%\n",
            "Test Precision: 68.88%\n",
            "Test Recall: 67.16%\n",
            "Test F1: 68.01%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "# Load and preprocess the validation data\n",
        "image_paths = []  # List of image file paths\n",
        "labels = []  # List of corresponding labels (0 for cats, 1 for dogs)\n",
        "\n",
        "for img_path in os.listdir(validation_cats_dir):\n",
        "    image = cv2.imread(os.path.join(validation_cats_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.resize(image, (150, 150))  # Resize the image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
        "        features = extract_hog_features(image)  # Extract HOG features\n",
        "        image_paths.append(features)\n",
        "        labels.append(0)  # 0 represents cats\n",
        "\n",
        "for img_path in os.listdir(validation_dogs_dir):\n",
        "    image = cv2.imread(os.path.join(validation_dogs_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.resize(image, (150, 150))  # Resize the image\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if needed\n",
        "        features = extract_hog_features(image)  # Extract HOG features\n",
        "        image_paths.append(features)\n",
        "        labels.append(1)  # 1 represents dogs\n",
        "\n",
        "\n",
        "# Convert the image_paths and labels lists to NumPy arrays\n",
        "X_validation = np.array(image_paths)\n",
        "y_validation = np.array(labels)\n",
        "\n",
        "# Use your trained Logistic Regression model to make predictions on the validation data\n",
        "y_validation_pred = pipeline.predict(X_validation)\n",
        "\n",
        "# Calculate metrics for the validation data\n",
        "accuracy_validation = accuracy_score(y_validation, y_validation_pred)\n",
        "precision_validation = precision_score(y_validation, y_validation_pred)\n",
        "recall_validation = recall_score(y_validation, y_validation_pred)\n",
        "f1_validation = f1_score(y_validation, y_validation_pred)\n",
        "\n",
        "print(f\"Validation Accuracy: {accuracy_validation * 100:.2f}%\")\n",
        "print(f\"Validation Precision: {precision_validation * 100:.2f}%\")\n",
        "print(f\"Validation Recall: {recall_validation * 100:.2f}%\")\n",
        "print(f\"Validation F1: {f1_validation * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ieOJZ6nuTFv",
        "outputId": "bb634f28-c80a-4a54-a81f-a28f46c48d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 68.10%\n",
            "Validation Precision: 66.98%\n",
            "Validation Recall: 71.40%\n",
            "Validation F1: 69.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MENGGUNAKAN FEATURE EXTRACTION LBP"
      ],
      "metadata": {
        "id": "ooM9gQ6_NREA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from skimage.feature import local_binary_pattern\n",
        "from skimage import exposure"
      ],
      "metadata": {
        "id": "0qyX2LFhO4cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract LBP features from an image\n",
        "def extract_lbp_features(image):\n",
        "    radius = 1\n",
        "    n_points = 8 * radius\n",
        "    lbp_image = local_binary_pattern(image, n_points, radius, method='uniform')\n",
        "    lbp_hist, _ = np.histogram(lbp_image.ravel(), bins=np.arange(0, n_points + 3), range=(0, n_points + 2))\n",
        "    lbp_hist = lbp_hist.astype(\"float\")\n",
        "    lbp_hist /= (lbp_hist.sum() + 1e-6)\n",
        "    return lbp_hist\n",
        "\n",
        "# Load and preprocess the data\n",
        "image_paths = []  # List of image file paths\n",
        "labels = []  # List of corresponding labels (0 for cats, 1 for dogs)\n",
        "\n",
        "for img_path in os.listdir(train_cats_dir):\n",
        "    image = cv2.imread(os.path.join(train_cats_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "        features = extract_lbp_features(image)  # Extract LBP features\n",
        "        image_paths.append(features)\n",
        "        labels.append(0)  # 0 represents cats\n",
        "\n",
        "for img_path in os.listdir(train_dogs_dir):\n",
        "    image = cv2.imread(os.path.join(train_dogs_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "        features = extract_lbp_features(image)  # Extract LBP features\n",
        "        image_paths.append(features)\n",
        "        labels.append(1)  # 1 represents dogs\n",
        "\n",
        "# Convert the image_paths and labels lists to NumPy arrays\n",
        "X = np.array(image_paths)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(C=0.1, max_iter=1000, penalty='l1', solver='saga')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('classifier', logistic_regression)\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate metrics for the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Test Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Test Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Test F1: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58bBOK-fNTrr",
        "outputId": "36f704c8-180f-4d59-ef21-2d6fd1ecdc05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 63.25%\n",
            "Test Precision: 62.16%\n",
            "Test Recall: 68.66%\n",
            "Test F1: 65.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the validation data\n",
        "image_paths = []  # List of image file paths\n",
        "labels = []  # List of corresponding labels (0 for cats, 1 for dogs)\n",
        "\n",
        "for img_path in os.listdir(validation_cats_dir):\n",
        "    image = cv2.imread(os.path.join(validation_cats_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "        features = extract_lbp_features(image)  # Extract LBP features\n",
        "        image_paths.append(features)\n",
        "        labels.append(0)  # 0 represents cats\n",
        "\n",
        "for img_path in os.listdir(validation_dogs_dir):\n",
        "    image = cv2.imread(os.path.join(validation_dogs_dir, img_path))\n",
        "    if image is not None:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
        "        features = extract_lbp_features(image)  # Extract LBP features\n",
        "        image_paths.append(features)\n",
        "        labels.append(1)  # 1 represents dogs\n",
        "\n",
        "# Convert the image_paths and labels lists to NumPy arrays\n",
        "X_validation = np.array(image_paths)\n",
        "y_validation = np.array(labels)\n",
        "\n",
        "# Use your trained Logistic Regression model to make predictions on the validation data\n",
        "y_validation_pred = pipeline.predict(X_validation)\n",
        "\n",
        "# Calculate metrics for the validation data\n",
        "accuracy_validation = accuracy_score(y_validation, y_validation_pred)\n",
        "precision_validation = precision_score(y_validation, y_validation_pred)\n",
        "recall_validation = recall_score(y_validation, y_validation_pred)\n",
        "f1_validation = f1_score(y_validation, y_validation_pred)\n",
        "\n",
        "print(f\"Validation Accuracy: {accuracy_validation * 100:.2f}%\")\n",
        "print(f\"Validation Precision: {precision_validation * 100:.2f}%\")\n",
        "print(f\"Validation Recall: {recall_validation * 100:.2f}%\")\n",
        "print(f\"Validation F1: {f1_validation * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlYpZyx7NZhW",
        "outputId": "4c0f1c48-c073-4317-c4d1-67236d3d6974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 50.00%\n",
            "Validation Precision: 50.00%\n",
            "Validation Recall: 100.00%\n",
            "Validation F1: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MENGGUNAKAN FEATURE EXTRACTION SIFT\n"
      ],
      "metadata": {
        "id": "MZMoHJXPOo_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install opencv-contrib-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQLsGcZjPFeq",
        "outputId": "d38bfa2f-ab78-4676-d335-d20e03965587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-contrib-python) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MiniBatchKMeans"
      ],
      "metadata": {
        "id": "Jlf7ao2FOrxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract SIFT features from an image\n",
        "def extract_sift_features(image, sift):\n",
        "    keypoints, descriptors = sift.detectAndCompute(image, None)\n",
        "    return keypoints, descriptors\n",
        "\n",
        "# Load and preprocess the data\n",
        "image_paths = []  # List of image file paths\n",
        "labels = []  # List of corresponding labels (0 for cats, 1 for dogs)\n",
        "\n",
        "for img_path in os.listdir(train_cats_dir):\n",
        "    image = cv2.imread(os.path.join(train_cats_dir, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is not None:\n",
        "        image_paths.append(image)\n",
        "        labels.append(0)  # 0 represents cats\n",
        "\n",
        "for img_path in os.listdir(train_dogs_dir):\n",
        "    image = cv2.imread(os.path.join(train_dogs_dir, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is not None:\n",
        "        image_paths.append(image)\n",
        "        labels.append(1)  # 1 represents dogs\n",
        "\n",
        "# Create SIFT extractor\n",
        "sift = cv2.xfeatures2d.SIFT_create()\n",
        "\n",
        "# Extract SIFT features from images\n",
        "keypoints_list = []\n",
        "descriptors_list = []\n",
        "\n",
        "for image in image_paths:\n",
        "    keypoints, descriptors = extract_sift_features(image, sift)\n",
        "    keypoints_list.extend(keypoints)\n",
        "    descriptors_list.extend(descriptors)\n",
        "\n",
        "# Convert descriptors to a NumPy array\n",
        "descriptors_array = np.array(descriptors_list)\n",
        "\n",
        "# Create a KMeans model with a fixed cluster size\n",
        "desired_length = 128  # Adjust as needed\n",
        "kmeans = KMeans(n_clusters=desired_length, random_state=42)\n",
        "kmeans.fit(descriptors_array)\n",
        "\n",
        "# Create histograms of visual words for training data\n",
        "X_train_histograms = []\n",
        "\n",
        "for image in image_paths:\n",
        "    _, descriptors = extract_sift_features(image, sift)\n",
        "    cluster_assignments = kmeans.predict(descriptors)\n",
        "    histogram = np.bincount(cluster_assignments, minlength=desired_length)\n",
        "    X_train_histograms.append(histogram)\n",
        "\n",
        "X_train_histograms = np.vstack(X_train_histograms)\n",
        "\n",
        "# Convert the labels to a NumPy array\n",
        "y = np.array(labels)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train_histograms, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression model\n",
        "logistic_regression = LogisticRegression(C=0.1, max_iter=1000, penalty='l1', solver='saga')\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('classifier', logistic_regression)\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Calculate metrics for the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Test Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Test Recall: {recall * 100:.2f}%\")\n",
        "print(f\"Test F1: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9v3jVWxO_2C",
        "outputId": "9da717cc-5797-42fc-a0ea-dfd98ddbe7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 69.25%\n",
            "Test Precision: 67.73%\n",
            "Test Recall: 74.13%\n",
            "Test F1: 70.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the validation set\n",
        "\n",
        "# Load and preprocess the validation data\n",
        "validation_image_paths = []  # List of image file paths for validation\n",
        "validation_labels = []  # List of corresponding labels (0 for cats, 1 for dogs) for validation\n",
        "\n",
        "for img_path in os.listdir(validation_cats_dir):\n",
        "    image = cv2.imread(os.path.join(validation_cats_dir, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is not None:\n",
        "        validation_image_paths.append(image)\n",
        "        validation_labels.append(0)  # 0 represents cats\n",
        "\n",
        "for img_path in os.listdir(validation_dogs_dir):\n",
        "    image = cv2.imread(os.path.join(validation_dogs_dir, img_path), cv2.IMREAD_GRAYSCALE)\n",
        "    if image is not None:\n",
        "        validation_image_paths.append(image)\n",
        "        validation_labels.append(1)  # 1 represents dogs\n",
        "\n",
        "# Extract SIFT features from validation images\n",
        "validation_keypoints_list = []\n",
        "validation_descriptors_list = []\n",
        "\n",
        "for image in validation_image_paths:\n",
        "    keypoints, descriptors = extract_sift_features(image, sift)\n",
        "    validation_keypoints_list.extend(keypoints)\n",
        "    validation_descriptors_list.extend(descriptors)\n",
        "\n",
        "# Convert descriptors to a NumPy array for validation data\n",
        "validation_descriptors_array = np.array(validation_descriptors_list)\n",
        "\n",
        "# Create histograms of visual words for validation data\n",
        "X_validation_histograms = []\n",
        "\n",
        "for image in validation_image_paths:\n",
        "    _, descriptors = extract_sift_features(image, sift)\n",
        "    cluster_assignments = kmeans.predict(descriptors)\n",
        "    histogram = np.bincount(cluster_assignments, minlength=desired_length)\n",
        "    X_validation_histograms.append(histogram)\n",
        "\n",
        "X_validation_histograms = np.vstack(X_validation_histograms)\n",
        "\n",
        "# Convert the labels to a NumPy array for validation data\n",
        "y_validation = np.array(validation_labels)\n",
        "\n",
        "# Predict using the trained model\n",
        "y_validation_pred = pipeline.predict(X_validation_histograms)\n",
        "\n",
        "# Calculate metrics for the validation set\n",
        "validation_accuracy = accuracy_score(y_validation, y_validation_pred)\n",
        "validation_precision = precision_score(y_validation, y_validation_pred)\n",
        "validation_recall = recall_score(y_validation, y_validation_pred)\n",
        "validation_f1 = f1_score(y_validation, y_validation_pred)\n",
        "\n",
        "# Print validation metrics\n",
        "print(f\"Validation Accuracy: {validation_accuracy * 100:.2f}%\")\n",
        "print(f\"Validation Precision: {validation_precision * 100:.2f}%\")\n",
        "print(f\"Validation Recall: {validation_recall * 100:.2f}%\")\n",
        "print(f\"Validation F1: {validation_f1 * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "9tQ8dUaFpxUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015439eb-7d1e-4200-9463-530ce798874b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 68.70%\n",
            "Validation Precision: 67.03%\n",
            "Validation Recall: 73.60%\n",
            "Validation F1: 70.16%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#KESIMPULAN"
      ],
      "metadata": {
        "id": "A4HZxAoTtQyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kesimpulannya, dengan menggunakan feature extraction dapat meningkatkan akurasi daripada yang tidak menggunakan feature extraction. Namun hal ini juga tergantung metode yang digunakan, bisa dilihat bahwa menggunakan feature extraction LBP tidak mengubah akurasi yang begitu banyak jika dibandingkan dengan tanpa menggunakan feature extraction"
      ],
      "metadata": {
        "id": "ZjlHnZ015wr-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KCh3lOw7_MH7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}